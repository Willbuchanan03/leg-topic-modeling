# -*- coding: utf-8 -*-
"""Initial Topic Modeling Attempt.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TElxym10nTY4TVdD6P3PrKnmBmNt18yu
"""

from google.colab import drive
drive.mount('/content/drive')

# MODULES AND LIBRARIES --------------------------------------------------------
from google.colab import files
from operator import index
import requests
import numpy as np
from gensim import corpora, models, similarities
from gensim.similarities import MatrixSimilarity
from gensim.parsing.preprocessing import preprocess_string, STOPWORDS, \
remove_stopwords, preprocess_documents
import gensim.utils
import time
import getpass
import os
import re

# USER SPECIFICATIONS ----------------------------------------------------------
# You can replace this path with a path to a folder of your choice, assuming you
# have mounted your google drive. This can also operate off of folders on your
# local drive. If you are selecting your own folder, thee program will either
# create and populate the files pl_list.txt does not exist or if it is empty.
txt_path = '/content/drive/MyDrive/Thesis_Raw_Data/pl_list.txt'
num_path = '/content/drive/MyDrive/Thesis_Raw_Data/num_list.txt'

# API Keys can be signed up for at gpo.congress.gov, and as a good practice,
# be hard-coded into code.

print('API Keys can be signed up for at gpo.congress.gov')
api_key = getpass.getpass(prompt="Enter your Congress.gov API key: ")

# Modify start and end values for the range of congresses you want to query
# WARNING: Summaries are only available beginning with the 93rd Congress (1973)
start = 93
end = 94

# Words to exclude from your lists besides the Gensim defaults
bad_words = [
    'the', 'bill', 'or',
    'to', 'and', 'in',
    'sec', 'any', 'by',
    'such', 'as', 'act'
]

# Enter selected bill number with formatting "congress#-chamber_bill"
selected_bill_numbers = [

# Bill to Amend the Economic Growth, Regulatory Relief, and Consumer Protection Act
    '117-s_3409',
# Dodd-Frank
    '111-hr_4173',
# Gramm-Leach-Bliley
    '106-s_900',
# Riegle-Neal Interstate Banking Act
    '103-hr_3841',
# FIRREA
    '101-hr_1278',
# Garn-St. Germain Depository Institutions Act
    '97-hr_6267',
# Depository Institutions Deregulation and Monetary Control Act
    '96-hr_4986'

]

# FIXED INPUTS -----------------------------------------------------------------

api_base_url = "https://api.congress.gov/v3/bill"
api_pl_url = "https://api.congress.gov/v3/law"

# Generated range of congresses to pull data from.
congresses = range(start, end + 1)

# Add spaces such that stop only clips full words and not parts of words
custom_stop = []
for c in bad_words:
  custom_stop.append('\x20' + c + '\x20')

# Rate Limiting to Comply with use requirements
API_RATE_LIMIT_REQUESTS = 5000
API_RATE_LIMIT_SECONDS = 3600 / API_RATE_LIMIT_REQUESTS

# Empty Lists
cleaned = []
selected_summaries = []
selected_billnums = []
clean_selected = []
pl_summaries = []
pl_nums = []
clean_pls = []

# FUNCTION DEFINITIONS ---------------------------------------------------------

# FETCHING SUMMARIES FROM CONGRESS API
def fetch_selected_summaries(congress, formatted_bill, api_key):
  ''' Takes input of a list of target congresses, the formatted list of bills
  and the user's API key, and returns summaries and bill numbers for the
  selected summaries.
  '''
  api_url = f"{api_base_url}/{congress}/{formatted_bill}/summaries"
  params = {"api_key": api_key, "format": "json"}
  response = requests.get(api_url, params=params)
  # If API Returns good response
  if response.status_code == 200:
      try:
          data = response.json()
          summaries = []
          bill_nums = []

          for summary in data.get("summaries", []):

            text = summary.get("text", "").strip()
            for c in custom_stop:
                text = text.replace(c,'')
            bill_number = summary.get("billNumber", "")
            if text:
              summaries.append(text)
              bill_nums.append(bill_number)
          return summaries, bill_nums
      except ValueError:
          print("Error Parsing JSON")
          return []
  else:
      print(f"Failed to retrieve data for Congress {congress}, Bill {formatted_bill}. HTTP {response.status_code}")
      return []
  time.sleep(0.72)

# Function to fetch summaries of all PLs from a range of congresses
def fetch_public_law_summaries(congress, api_key):
  ''' Takes a list of target congresses and the users API key as a string
  and returns a list of summaries and their associated Public Law
  identifiers.
  '''
  api_url = f"{api_pl_url}/{congress}/pub"
  params = {"api_key": api_key, "format": "json", "limit": 250, "offset": 0}
  summaries = []
  pl_nums = []

  while True:
      try:
          response = requests.get(api_url, params=params)
          print(f"Fetching public laws from {response.url}")
          response.raise_for_status()  # Raise exception for bad status codes
          data = response.json()

          # Check if results are empty
          if "bills" not in data or not data["bills"]:
              print("No more results, stopping iteration.")
              break

          bill_urls = []
          for pl in data.get("bills", []):
              try:
                  for laws in pl.get("laws", []):
                      pl_num = laws.get("number", "")
                      if pl_num:
                          pl_nums.append(pl_num)

                  bill_url = pl.get("url", "")
                  bill_urls.append(bill_url)

                  for bill_url in bill_urls:
                      bill_url, url_params = bill_url.split('?')
                      summary_url = f"{bill_url}/summaries?{url_params}"
                      time.sleep(0.72)
                      law_response = requests.get(summary_url, params={"api_key": api_key})
                      law_response.raise_for_status()

                      law_data = law_response.json()
                      req = law_data.get('request', {})
                      law_info = f"|{req.get('congress')}{req.get('billType')}{req.get('billNumber')}|"

                      summary_text = None
                      for summary in law_data.get("summaries", []):
                          action_desc = summary.get("actionDesc")
                          if action_desc == "Public Law":
                              summary_text = summary.get("text", "")
                              break
                          elif action_desc in ["Passed Senate amended", "Passed Senate",
                                                "Senate agreed to House amendment with amendment"]:
                              summary_text = summary.get("text", "")

                      if summary_text:
                          # Clean the summary text
                          for stop_word in custom_stop:
                              summary_text = summary_text.replace(stop_word, ' ')
                          summaries.append(f"|{summary_text}|")
                          pl_nums.append(law_info)

              except Exception as e:
                  print(f"Error processing bill: {e}")
                  continue

          # Move to the next batch
          params["offset"] += 250

      except Exception as e:
          print(f"Failed to retrieve data for Congress {congress}: {e}")
          break

  return summaries, pl_nums


def store_public_law_data(pl_summaries, pl_nums, txt_path, num_path):
  ''' Takes list of summaries, string of a file path to store the summaries
  of the public laws, and a string of the path to store Public Law Numbers
  for each bill. Does not return anything, but stores summaries into txt files.
  '''
  try:
      # Store summaries
      with open(txt_path, 'w', encoding='utf-8') as f:
          for pl_list in pl_summaries:
              for summary in pl_list:
                  f.write(f"{summary}\n")

      # Store numbers
      with open(num_path, 'w', encoding='utf-8') as fn:
          for num_list in pl_nums:
              for num in num_list:
                  fn.write(f"{num}\n")

      print(f"Successfully stored {sum(len(pl) for pl in pl_summaries)} summaries")
      print(f"Successfully stored {sum(len(nums) for nums in pl_nums)} numbers")

  except Exception as e:
      print(f"Error storing data: {e}")

def retrieve_txt(path):
  '''Takes any path and retrieves information stored at the txt file indicated
  by the path
  '''
  biglist = []
  with open(path, 'r') as f1:
    content = f1.read()
  lists = re.findall(r"\|.*?\|", content)
  return lists

# DATA PROCESSING

def clean_summary(docs):
  '''Takes list of summaries as input and preprocesses documents.
  Returns list of tokenized and cleaned summaries.
  '''
  csum = preprocess_documents(docs)
  return csum

# Function to generate dictionaries
def generate_dictionary(docs):
  ''' Takes some list of docs as input and returns a dictionary that maps
  each word to some key value.
  '''
  dictionary = corpora.Dictionary(docs)
  return dictionary

# Function to generate corpus
def generate_corpus(dictionary, docs):
  '''Takes a dictionary and list of summaries and returns a 'Bag of Words',
  where each key in the dictionary receives an attached frequency count for
  the amount of times it appears in that doc. Returns a Bag of Words for each
  doc in docs, together a a list, corpus.
  '''
  corpus = [dictionary.doc2bow(doc) for doc in docs]
  return corpus


# Function to train LSI
def train_lsi(corpus, dictionary, num_topics = 20):
  ''' Takes a corpus, a key-word dictionary, and a number of topics, and returns
  an LSI model trained on that corpus.
  '''
  lsi_model = models.LsiModel(corpus, id2word=dictionary, num_topics=num_topics)
  return lsi_model

# Function to Generate LSI
def generate_lsi(model, bow):
  '''Takes a model and a bag of words, and returns the topic results of
  passing that bag of words through the model.
  '''
  lsi_out =  model[bow]
  return lsi_out

# Function to print LSI topics
def print_lsi_topics(lsi_model, num_topics=10):
  '''Takes a processed model output and a number of topics, and prints the 'top'
  n topics.
  '''
  print("LSI Topics:")
  for topic_id, topic in lsi_model.print_topics(num_topics=num_topics):
      print(f"Topic {topic_id}: {topic}")

# Creating List from Retrieved Text
def txt_to_list(retrieved_txt):
  ''' Takes raw retrieved string from text and separates it into individual
  summaries stored in a list, then returns that list.
  '''
  docs = []
  for item in retrieved_txt:
    ret = eval(item)
    docs.append(ret)
  return docs

# CONSTRUCTING AND APPLYING MODEL

def create_matrix(ref_corpus_bow, ref_dict):
  '''Takes a corpus and dictionary created from the selected legislation
  and returns a matrix used to compare similar elements of other bills.
  '''
  similarity_matrix = MatrixSimilarity(ref_corpus_bow, num_features=len(ref_dict))
  return similarity_matrix

# Create baseline matrix using selected corpus to benchmark other similarities
def compute_baseline_similarity(selected_corpus, selected_dict):
  '''Takes the PL summaries from range of congresses and the dictionary
  generated from those summaries, and creates a similarity matrix.
  Returns the similarity threshold used to identify related bills and the
  list of related bills.
  '''
  sim_matrix = create_matrix(selected_corpus, selected_dict)
  # Compute individual document similarities within corpus
  doc_similarities = []
  for i, doc in enumerate(selected_corpus):
      doc_sims = [sim[i] for sim in find_similar(sim_matrix, selected_corpus)]
      avg_doc_sim = np.mean(doc_sims)
      doc_similarities.append(avg_doc_sim)

  # Use median or mean as threshold
  baseline_threshold = np.median(doc_similarities)
  return baseline_threshold, doc_similarities

# Returning the Similarity of each doc bow to each topic of the reference corpus
def find_similar(similarity_matrix, retrieved_corpus_bow):
  '''Takes the similarity matrix and the corpus of retrieved documents.
  Returns the similarities of the selected baseline documents.
  '''
  sims = []
  for bow in retrieved_corpus_bow:
    sim = similarity_matrix[bow]
    sims.append(sim)
  return sims

# FUNCTIONALITY ----------------------------------------------------------------

# DATA COLLECTION
# Reformatting selected bill information to put in API request
for item in selected_bill_numbers:
    if '-' in item and '_' in item:
        congress, info_only = item.split('-')
        chamber, bill_number = info_only.split('_')
        chamber = chamber.lower()
        if chamber in ['hr', 's'] and bill_number.isdigit():
            formatted_bill = f"{chamber}/{bill_number}"
            cleaned.append((congress, formatted_bill))
        else:
            print("Invalid format.")
    else:
        print("Invalid format.")

# Generating List of Raw Summaries and Cleaning, Storing Them.
# If they have already been stored, it does not attempt to fetch again.
if not os.path.exists(txt_path) or os.stat(txt_path).st_size == 0:
    pl_summaries = []
    pl_nums = []

    for congress in congresses:
        summaries, nums = fetch_public_law_summaries(congress, api_key)
        if summaries:  # Only append if we got data
            pl_summaries.append(summaries)
            pl_nums.append(nums)
        else:
          print("We didn't get data, dumbass")

    store_public_law_data(pl_summaries, pl_nums, txt_path, num_path)

# Retrieve Summaries from Selected Bills
for congress, formatted_bill in cleaned:
  selected_summary, bill_num = fetch_selected_summaries(congress, formatted_bill, api_key)
  selected_summaries.append(selected_summary)


# DATA RECOVERY
# Reopening text of PLs and Associated Bill Numbers
pl_txt = retrieve_txt(txt_path)
num_txt = retrieve_txt(num_path)

# DATA PROCESSING

# Clean and Tokenize Summaries of Selected Bills
for summary in selected_summaries:
  csum = clean_summary(summary)
  clean_selected.extend(csum)

# Turn Tokenized Selected Summaries into Dictionary
selected_dict = generate_dictionary(clean_selected)

# Use Dictionary to Create Corpus of Selected Bills
selected_corpus = generate_corpus(selected_dict, clean_selected)

# Training Model on Selected Bill Corpus and Dictionary
trained_model = train_lsi(selected_corpus,selected_dict)

# Use Training Model, Corpus BOW to create vectorized space model of Selected Corpus
selected_lsi = generate_lsi(trained_model, selected_corpus)

# Taking each stored list and convert from str to list
c_pl = clean_summary(pl_txt)

# Create a pl_dict
pl_dict = generate_dictionary(c_pl)

# Create a single dictionary from all preprocessed texts
common_dict = selected_dict
common_dict.merge_with(pl_dict)

# Separate corpora using common dictionary
selected_corpus = generate_corpus(common_dict, clean_selected)
pl_bow = generate_corpus(common_dict, c_pl)

# ACTUAL COMPARISON -----------------------------------------------------------
# Creating matrix for selected bills to compare docs to
sim_matrix = create_matrix(selected_corpus, common_dict)

# Calculates the 'baseline' similarity of selected bills to each other.
baseline_threshold, doc_similarities = compute_baseline_similarity(selected_corpus, selected_dict)

# Creates list of similarity scores for each doc in pl_bow
sim_scores = find_similar(sim_matrix, pl_bow)

# Returns the scores of documents and their bill information while they are
# Above the baseline threshold
for i, sim in enumerate(sim_scores):
    avg_sim = sum(sim) / len(sim)
    if avg_sim > baseline_threshold:
        # Extract the actual bill number/details from num_txt[i]
        bill_details = num_txt[i].strip('|')
        print(f"Document {bill_details} has average similarity of {avg_sim:.4f} to reference topics")