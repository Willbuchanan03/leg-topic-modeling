# -*- coding: utf-8 -*-
"""OptimizedTopicModeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1904Hfy8W-Hv60d0-eZlEYR4jpa0zREC2
"""

from IPython import display
!pip install requests colorlog pandas numpy scipy gensim networkx pyvis nltk tqdm
display.clear_output()
import nltk
# Run program, input 'd' then 'all'. Once download is completed, enter 'q'
nltk.download()

import requests
import json
import time
import logging
import colorlog
import urllib3
import random
from dataclasses import dataclass
from datetime import datetime
import getpass
from pathlib import Path
import numpy as np
from gensim import corpora, models
from gensim.models import LsiModel
import pandas as pd
from scipy.spatial.distance import jensenshannon
from scipy.cluster.hierarchy import linkage, fcluster
from tqdm import tqdm
import networkx as nx
from pyvis.network import Network
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from gensim.parsing.preprocessing import STOPWORDS as GENSIM_STOPS
from typing import List, Set, Dict, Any, Tuple, Optional, Callable
from collections import Counter, defaultdict
import matplotlib.pyplot as plt
import re
import os

class RateLimitHandler:
    """
    Handles rate limiting and exponential backoff for API requests.
    """
    def __init__(self,
                 base_delay: float = 1.0,
                 max_delay: float = 60.0,
                 max_retries: int = 5):
        """
        Initialize rate limit handler.

        :param base_delay: Initial delay between retries (in seconds)
        :param max_delay: Maximum delay between retries (in seconds)
        :param max_retries: Maximum number of retry attempts
        """
        self.base_delay = base_delay
        self.max_delay = max_delay
        self.max_retries = max_retries

    def make_request_with_backoff(self,
                                  request_func: Callable[[], requests.Response],
                                  logger: Optional[logging.Logger] = None) -> Optional[requests.Response]:
        """
        Make an API request with exponential backoff for rate limiting.

        :param request_func: Function that makes the API request
        :param logger: Optional logger for tracking retry attempts
        :return: Response object or None if all retries fail
        """
        current_delay = self.base_delay

        for attempt in range(self.max_retries + 1):
            try:
                response = request_func()

                # Successful non-rate limit response
                if response.status_code != 429:
                    return response

                # Handle rate limit response
                if logger:
                    logger.warning(f"Rate limit hit. Attempt {attempt + 1}/{self.max_retries}")

                # If it's the last attempt, return the rate limit response
                if attempt == self.max_retries:
                    return response

                # Calculate delay with jitter to prevent thundering herd problem
                jitter = random.uniform(0, 0.1 * current_delay)
                sleep_time = min(current_delay + jitter, self.max_delay)

                if logger:
                    logger.info(f"Waiting {sleep_time:.2f} seconds before retry")

                time.sleep(sleep_time)

                # Exponential backoff with full jitter
                current_delay *= 2

            except requests.RequestException as e:
                if logger:
                    logger.error(f"Request failed: {e}")

                # If it's the last attempt, re-raise or return None
                if attempt == self.max_retries:
                    if logger:
                        logger.error("All retry attempts failed")
                    return None

                # Wait before retrying network errors
                time.sleep(current_delay)
                current_delay *= 2

        return None

class RateLimitPauseHandler:
    """
    Manages rate limit pauses with a progress bar and intelligent caching.
    """
    def __init__(self,
                 pause_duration: int = 60,
                 max_pauses: int = 3,
                 logger: Optional[logging.Logger] = None):
        """
        Initialize rate limit pause handler.

        :param pause_duration: Duration of pause in seconds
        :param max_pauses: Maximum number of pauses before giving up
        :param logger: Optional logger for tracking pauses
        """
        self.pause_duration = pause_duration
        self.max_pauses = max_pauses
        self.logger = logger or logging.getLogger(__name__)
        self.pause_count = 0

    def pause_with_progress(self):
        """
        Pause execution with a progress bar.

        :return: True if pause was successful, False if max pauses exceeded
        """
        if self.pause_count >= self.max_pauses:
            self.logger.error("Maximum rate limit pauses exceeded. Stopping.")
            return False

        self.pause_count += 1

        try:
            # Attempt to import tqdm for progress bar
            from tqdm import tqdm

            self.logger.warning(f"Rate limit hit. Pausing for {self.pause_duration} seconds.")

            # Create progress bar
            for _ in tqdm(
                range(self.pause_duration),
                desc=f"Rate Limited (Pause {self.pause_count}/{self.max_pauses})",
                bar_format="{l_bar}{bar}",
                ncols=70
            ):
                time.sleep(1)

            return True

        except ImportError:
            # Fallback if tqdm is not available
            self.logger.warning(f"Rate limit hit. Pausing for {self.pause_duration} seconds without progress bar.")
            time.sleep(self.pause_duration)
            return True

@dataclass
class BillSummary:
    """Represents a bill summary with metadata."""
    congress: str
    bill_number: str
    summary_text: str
    year: int

@dataclass
class ProcessedDocument:
    """Represents a processed legislative document."""
    original_id: str
    year: int
    tokens: List[str]
    summary_text: str = ''
    bow: Any = None
    lsi_vec: Any = None
    topic_dist: Any = None

class LegislativeAnalyzer:
    """Main class combining fetching, processing, and visualization."""

    def __init__(self, api_key: str,
             cache_dir: Optional[str] = "/content/drive/MyDrive/Thesis_Raw_Data/Cache_Data",  # Default to Google Drive path
             random_seed: int = 42):
        """
        Initialize the analyzer with Google Drive cache support.

        :param api_key: Congress.gov API key
        :param cache_dir: Cache directory path, defaults to Google Drive location
        :param random_seed: Random seed for reproducible results
        """
        # Store random seed
        self.random_seed = random_seed

        # Set random seeds for all sources of randomness
        self._set_random_seed(random_seed)

        # Initialize logging
        self.logger = self._setup_logger()
        urllib3_logger = logging.getLogger('urllib3')
        urllib3_logger.setLevel(logging.WARNING)

        # API configuration
        self.api_key = api_key
        self.API_BASE_URL = "https://api.congress.gov/v3"
        self.RATE_LIMIT_DELAY = 0.72

        # Initialize paths with proper structure
        self.data_dir = Path(cache_dir)
        self.raw_cache_path = self.data_dir / 'bill_data.json'  # Keep original filename
        self.analysis_cache_dir = self.data_dir / 'analysis_results'

        # Create necessary directories
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.analysis_cache_dir.mkdir(parents=True, exist_ok=True)

        # Log cache locations
        self.logger.info(f"Raw cache path: {self.raw_cache_path}")
        self.logger.info(f"Analysis cache directory: {self.analysis_cache_dir}")

        # Initialize text processing
        self._setup_text_processing()

        # Add rate limit handlers
        self.rate_limit_handler = RateLimitHandler(
            base_delay=self.RATE_LIMIT_DELAY,
            max_delay=60.0,
            max_retries=5
        )
        self.rate_limit_pause_handler = RateLimitPauseHandler(
            pause_duration=60,
            max_pauses=3,
            logger=self.logger
        )

        # Track processed congresses to support incremental caching
        self.processed_congresses = set()

    def _set_random_seed(self, seed: int) -> None:
        """
        Set random seed for all random number generators.

        :param seed: Random seed value
        """
        np.random.seed(seed)
        random.seed(seed)

        # Set random seed for Python's hash function
        # This helps ensure consistent dictionary and set ordering
        import sys
        if sys.version_info[0] >= 3:
            hashseed = seed
            os.environ['PYTHONHASHSEED'] = str(hashseed)

    def _cache_raw_data(self, selected_summaries: List[BillSummary], public_law_summaries: List[BillSummary]) -> None:
        """Cache raw data independent of random seed."""
        cache_data = {
            'selected': [vars(s) for s in selected_summaries],
            'public_laws': [vars(s) for s in public_law_summaries],
            'cache_date': datetime.now().isoformat()
        }

        with self.raw_cache_path.open('w') as f:
            json.dump(cache_data, f, indent=2)
            self.logger.info(f"Cached {len(selected_summaries)} selected bills and {len(public_law_summaries)} public laws")

    def _read_cache(self) -> Optional[Dict]:
        """Read and validate cache file."""
        try:
            if self.raw_cache_path.exists():
                with self.raw_cache_path.open('r') as f:
                    return json.load(f)
        except Exception as e:
            self.logger.error(f"Error reading cache: {e}")
        return None

    def _validate_analysis_cache(self, cached_analysis: Dict) -> bool:
        """
        Validate analysis cache content.

        :param cached_analysis: Dictionary containing cached analysis results
        :return: Boolean indicating if cache is valid
        """
        try:
            required_keys = ['topics', 'related_documents', 'temporal_distribution']
            return all(key in cached_analysis for key in required_keys)
        except Exception as e:
            self.logger.error(f"Error validating analysis cache: {e}")
            return False

    def _get_analysis_cache_path(self) -> Path:
            """Get cache path for analysis results with current seed."""
            return self.analysis_cache_dir / f'analysis_{self.random_seed}.json'

    def _setup_logger(self) -> logging.Logger:
        """Configure colored logging."""
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)

        if not logger.handlers:
            handler = colorlog.StreamHandler()
            handler.setFormatter(colorlog.ColoredFormatter(
                fmt='%(log_color)s[%(levelname)s] %(message)s%(reset)s',
                log_colors={
                    'INFO': 'green',
                    'WARNING': 'yellow',
                    'ERROR': 'red',
                    'CRITICAL': 'red,bg_white',
                }
            ))
            handler.setLevel(logging.INFO)
            logger.addHandler(handler)

        return logger

    def _setup_text_processing(self):
        """
        Initialize text processing components with comprehensive NLTK resource download.
        Handles various potential download issues and provides robust error handling.
        """
        # Import necessary modules
        import ssl
        import os
        import warnings

        # Suppress specific warnings that might occur during download
        warnings.filterwarnings("ignore", category=UserWarning)

        # SSL context handling to prevent download issues
        try:
            _create_unverified_https_context = ssl._create_unverified_context
        except AttributeError:
            pass
        else:
            ssl._create_default_https_context = _create_unverified_https_context

        # Determine a safe download directory
        def get_safe_nltk_download_dir():
            # List of potential download directories
            potential_dirs = [
                os.path.join(os.getcwd(), 'nltk_data'),  # Current working directory
                os.path.expanduser('~/nltk_data'),       # User's home directory
                '/tmp/nltk_data'                         # Temporary directory
            ]

            # Find first writable directory
            for dir_path in potential_dirs:
                try:
                    os.makedirs(dir_path, exist_ok=True)
                    if os.access(dir_path, os.W_OK):
                        return dir_path
                except Exception:
                    continue

            # Fallback to default
            return None

        # Safe NLTK download directory
        safe_download_dir = get_safe_nltk_download_dir()
        if safe_download_dir:
            nltk.data.path.append(safe_download_dir)

        # List of NLTK resources to download
        nltk_resources = [
            'punkt',          # Tokenization
            'wordnet',        # Lemmatization
            'stopwords',      # Stop words
            'averaged_perceptron_tagger',  # Additional NLP resources
        ]

        # Comprehensive download attempt
        download_success = False
        download_attempts = 0
        max_download_attempts = 3

        while not download_success and download_attempts < max_download_attempts:
            try:
                # Attempt to download resources
                for resource in nltk_resources:
                    try:
                        # Use custom download directory if available
                        if safe_download_dir:
                            nltk.download(resource, download_dir=safe_download_dir, quiet=True)
                        else:
                            nltk.download(resource, quiet=True)

                        self.logger.info(f"Successfully downloaded NLTK resource: {resource}")
                    except Exception as resource_error:
                        self.logger.warning(f"Failed to download NLTK resource {resource}: {resource_error}")

                # Verify tokenization
                test_tokens = word_tokenize("This is a test sentence.")
                if not test_tokens:
                    raise ValueError("Tokenization failed after download")

                download_success = True

            except Exception as e:
                download_attempts += 1
                self.logger.error(f"NLTK download attempt {download_attempts} failed: {e}")

                # Wait a bit between attempts
                import time
                time.sleep(2)

        # If all download attempts fail
        if not download_success:
            self.logger.critical("NLTK resource download failed after multiple attempts.")
            print("\n--- NLTK Resource Download Instructions ---")
            print("Automatic download failed. Please try manually:")
            print("1. Open a Python interpreter")
            print("2. Run the following commands:")
            print("   import nltk")
            for resource in nltk_resources:
                print(f"   nltk.download('{resource}')")

            # Raise an informative error
            raise RuntimeError(
                "Unable to download NLTK resources. "
                "Please follow the manual download instructions above."
            )

        # Initialize lemmatizer and stopwords
        self.lemmatizer = WordNetLemmatizer()

        # Custom legislative stopwords
        self.legislative_stops = {
            'act', 'section', 'sec', 'subsection', 'paragraph', 'subparagraph',
            'clause', 'amendment', 'title', 'chapter', 'subtitle', 'part',
            'pursuant', 'provided', 'thereof', 'therein', 'herein', 'hereby',
            'whereas', 'notwithstanding', 'shall', 'may', 'bill', 'law',
            'public', 'congress', 'house', 'senate', 'representative', 'senator'
            # Procedural Terms
            'amends', 'authorizes', 'requires', 'establishes', 'provides', 'directs',
            'permits', 'prescribes', 'modifies', 'revises', 'implements', 'prohibits',
            'grants', 'extends', 'terminates', 'reauthorizes', 'appropriates', 'designates',
            # Legislative Structure Terms
            'section', 'subsection', 'paragraph', 'subparagraph', 'clause', 'title',
            'subtitle', 'chapter', 'subchapter', 'part', 'subpart', 'division',
            # Common Legal Phrases
            'pursuant', 'notwithstanding', 'whereas', 'provided', 'thereof', 'therein',
            'herein', 'hereby', 'foregoing', 'aforesaid', 'hereunder', 'thereto',
            'thereunder', 'hereinafter', 'hereinbefore', 'whatsoever', 'whosoever',
            # Document Types
            'bill', 'resolution', 'amendment', 'statute', 'regulation', 'rule',
            'ordinance', 'code', 'law', 'act', 'order', 'provision',
            # Government Terms
            'federal', 'state', 'local', 'municipal', 'government', 'agency',
            'department', 'secretary', 'administrator', 'director', 'commission',
            'committee', 'subcommittee', 'board', 'council', 'bureau',
            # Legislative Process
            'enacted', 'approved', 'effective', 'date', 'congressional', 'legislative',
            'administrative', 'regulatory', 'statutory', 'fiscal', 'calendar',
            # Common Auxiliary Words
            'shall', 'may', 'must', 'will', 'can', 'should', 'would', 'could',
            'such', 'any', 'each', 'every', 'all', 'none', 'either', 'neither',
            # Temporal Terms
            'date', 'year', 'month', 'day', 'period', 'term', 'duration', 'time',
            'deadline', 'timeline', 'schedule', 'fiscal', 'annual', 'quarterly'
                }

        # Combine stopwords
        self.stopwords = set(stopwords.words('english'))
        self.stopwords.update(GENSIM_STOPS)
        self.stopwords.update(self.legislative_stops)

    def _preprocess_text(self, text: str) -> List[str]:
        """Preprocess text."""
        text = text.lower()
        text = re.sub(r'\([a-zA-Z0-9]\)', ' ', text)
        text = re.sub(r'[^\w\s]', ' ', text)
        text = re.sub(r'\d+', ' ', text)
        tokens = word_tokenize(text)
        return [
            self.lemmatizer.lemmatize(token)
            for token in tokens
            if token not in self.stopwords and len(token) > 2
        ]

    def _create_corpus(self, documents: List[Tuple[str, int, str]]) -> List[ProcessedDocument]:
        """
        Create processed corpus from input documents.

        :param documents: List of tuples (document_id, year, text)
        :return: List of processed documents
        """
        return [
            ProcessedDocument(
                original_id=doc_id,
                year=year,
                tokens=self._preprocess_text(text),
                summary_text=text
            )
            for doc_id, year, text in documents
        ]

    def _build_dictionary(self, docs: List[ProcessedDocument]) -> corpora.Dictionary:
        """
        Build a Gensim dictionary from processed documents.

        :param docs: List of processed documents
        :return: Gensim Dictionary object
        """
        return corpora.Dictionary([doc.tokens for doc in docs])

    def _create_bow_corpus(self,
                           docs: List[ProcessedDocument],
                           dictionary: corpora.Dictionary) -> List[ProcessedDocument]:
        """
        Create bag-of-words representation for documents.

        :param docs: List of processed documents
        :param dictionary: Gensim Dictionary object
        :return: Updated list of processed documents with bow representation
        """
        for doc in docs:
            doc.bow = dictionary.doc2bow(doc.tokens)
        return docs

    def _train_lsi_model(self,
                    docs: List[ProcessedDocument],
                    dictionary: corpora.Dictionary,
                    num_topics: Optional[int] = None) -> LsiModel:
        """
        Train Latent Semantic Indexing (LSI) model with dynamic topic sizing.

        :param docs: List of processed documents
        :param dictionary: Gensim Dictionary object
        :param num_topics: Optional override for number of topics
        :return: Trained LSI model
        """
        corpus = [doc.bow for doc in docs]

        if num_topics is None:
            # Calculate number of topics based on number of input documents
            # Use minimum of (n_docs - 1) or 9 to ensure we don't overfit
            n_docs = len([doc for doc in docs if hasattr(doc, 'original_id') and doc.original_id.startswith('REF_')])
            num_topics = min(max(n_docs - 1, 1), 9)
            self.logger.info(f"Dynamically set number of topics to {num_topics} based on {n_docs} reference documents")

        return models.LsiModel(corpus, id2word=dictionary, num_topics=num_topics)

    def _apply_lsi(self,
                  docs: List[ProcessedDocument],
                  model: LsiModel) -> List[ProcessedDocument]:
        """
        Apply LSI transformation to documents.

        :param docs: List of processed documents
        :param model: Trained LSI model
        :return: Updated list of processed documents with LSI vectors
        """
        for doc in docs:
            doc.lsi_vec = model[doc.bow]
            doc.topic_dist = self._get_topic_distribution(doc.lsi_vec, model.num_topics)
        return docs

    def _get_topic_distribution(self,
                               lsi_vec: List[Tuple[int, float]],
                               num_topics: int) -> np.ndarray:
        """
        Convert LSI vector to normalized topic distribution.

        :param lsi_vec: LSI vector of (topic_id, weight) tuples
        :param num_topics: Total number of topics
        :return: Normalized topic distribution array
        """
        dist = np.zeros(num_topics)
        for topic_id, weight in lsi_vec:
            dist[topic_id] = weight
        return dist / (np.linalg.norm(dist) + 1e-10)

    def _congress_to_year(self, congress: int) -> int:
        """
        Convert congress number to its starting year.

        :param congress: Congress number
        :return: Starting year of the congress
        """
        return 1789 + ((congress - 1) * 2)

    def _congress_to_years(self, congress: int) -> Tuple[int, int]:
        """
        Convert congress number to its start and end years.

        :param congress: Congress number
        :return: Tuple of (start_year, end_year)
        """
        start_year = self._congress_to_year(congress)  # Use existing method for consistency
        end_year = start_year + 2
        return (start_year, end_year)

    def _is_cache_valid(self, cached_data: Dict, congresses: List[int], selected_bills: List[str]) -> bool:
        """
        Validate cache with selective invalidation of selected bills.

        :param cached_data: Dictionary containing cached data
        :param congresses: List of congress numbers to validate
        :param selected_bills: List of bill IDs to validate
        :return: Tuple[bool, Dict] - (is_valid, updated_cache_data)
        """
        try:
            # If no cached data, return False
            if not cached_data:
                self.logger.info("No cached data found")
                return False

            # Check if required keys exist
            required_keys = ['selected', 'public_laws']
            for key in required_keys:
                if key not in cached_data:
                    self.logger.info(f"Missing key: {key}")
                    return False

            # Get set of bills in cache
            cached_bills = {
                f"{s.get('congress', '')}-{s.get('bill_number', '')}"
                for s in cached_data.get('selected', [])
            }

            # Convert input bills to set
            selected_bills_set = set(selected_bills)

            # Check if bills match
            if cached_bills != selected_bills_set:
                self.logger.info("Selected bills do not match cache exactly")
                self.logger.info(f"Cached bills: {cached_bills}")
                self.logger.info(f"Selected bills: {selected_bills_set}")
                return False

            # Check congresses
            cached_congresses = {
                int(s.get('congress', 0))
                for s in cached_data.get('public_laws', [])
            }

            # Check if ALL required congresses are in the cache
            missing_congresses = set(congresses) - cached_congresses
            if missing_congresses:
                self.logger.info(f"Missing congresses in cache: {missing_congresses}")
                return False

            # Validate public laws data
            public_laws = cached_data.get('public_laws', [])
            if not public_laws:
                self.logger.info("No public laws in cache")
                return False

            # Additional validation for public laws
            for law in public_laws:
                if not all(key in law for key in ['congress', 'bill_number', 'summary_text', 'year']):
                    self.logger.info("Invalid public law entry found")
                    return False

            self.logger.info("Cache validation successful")
            return True

        except Exception as e:
            self.logger.error(f"Error in cache validation: {e}")
            import traceback
            traceback.print_exc()
            return False

    def fetch_data(self, congresses: List[int], selected_bills: List[str]) -> Tuple[List[BillSummary], List[BillSummary]]:
        """
        Fetch data with selective cache updating.

        :param congresses: List of congress numbers to analyze
        :param selected_bills: List of specific bills to analyze
        :return: Tuple of (selected_summaries, public_law_summaries)
        """
        selected_summaries = []
        public_law_summaries = []

        # Sort congresses to ensure consistent processing
        congresses = sorted(congresses)

        # Check for raw data cache first
        if self.raw_cache_path.exists():
            try:
                with self.raw_cache_path.open() as f:
                    cached_data = json.load(f)

                    print("Cache file found. Checking cache validity...")
                    is_valid = self._is_cache_valid(cached_data, congresses, selected_bills)

                    if is_valid:
                        print("Loading from raw data cache...")
                        selected_summaries = [BillSummary(**b) for b in cached_data['selected']]
                        public_law_summaries = [BillSummary(**b) for b in cached_data['public_laws']]

                        # Track already processed congresses
                        self.processed_congresses = set(int(s['congress']) for s in cached_data['public_laws'])

                        print(f"Loaded {len(selected_summaries)} selected bills")
                        print(f"Loaded {len(public_law_summaries)} public law bills")
                        print(f"Processed congresses: {self.processed_congresses}")

                        return selected_summaries, public_law_summaries
                    else:
                        # Preserve public laws data if it's valid
                        if cached_data.get('public_laws'):
                            print("Preserving existing public laws data...")
                            public_law_summaries = [BillSummary(**b) for b in cached_data['public_laws']]
                            self.processed_congresses = set(int(s['congress']) for s in cached_data['public_laws'])

            except Exception as e:
                print(f"Cache error: {e}")

        # Fetch selected bills
        self.logger.info("Fetching selected bills...")
        failed_bills = []
        for bill in tqdm(selected_bills, desc="Fetching bills"):
            # Skip if bill already in cache
            if any(b.bill_number == bill for b in selected_summaries):
                continue

            summary = self._fetch_bill_summary(bill)
            if summary:
                selected_summaries.append(summary)
            else:
                failed_bills.append(bill)
                self.logger.warning(f"Could not fetch summary for bill: {bill}")

        if failed_bills:
            self.logger.warning(f"Failed to fetch: {', '.join(failed_bills)}")

        # Only fetch public laws if we don't have them already
        if not public_law_summaries:
            self.logger.info("Fetching public laws...")
            for congress in tqdm(congresses, desc="Fetching public laws"):
                if congress in self.processed_congresses:
                    self.logger.info(f"Congress {congress} already processed. Skipping.")
                    continue

                try:
                    summaries = self._fetch_public_laws(congress)
                    if summaries is None:
                        if not self.rate_limit_pause_handler.pause_with_progress():
                            break
                        summaries = self._fetch_public_laws(congress)

                    if summaries:
                        public_law_summaries.extend(summaries)
                        self.processed_congresses.add(congress)

                except Exception as e:
                    self.logger.error(f"Error processing congress {congress}: {e}")

        # Cache raw data
        self._cache_raw_data(selected_summaries, public_law_summaries)

        return selected_summaries, public_law_summaries

    def _fetch_bill_summary(self, bill_info: str) -> Optional[BillSummary]:
        """
        Fetch summary for a single bill.

        :param bill_info: Bill information in format 'congress-chamber_number'
        :return: BillSummary object or None if fetch fails
        """
        try:
            congress, bill_data = bill_info.split('-')
            chamber, number = bill_data.split('_')
            chamber = chamber.lower()

            api_url = f"{self.API_BASE_URL}/bill/{congress}/{chamber}/{number}/summaries"
            params = {"api_key": self.api_key, "format": "json"}

            # Wrap the request in a function for rate limit handler
            def make_request():
                return requests.get(api_url, params=params)

            # Use rate limit handler to make the request
            response = self.rate_limit_handler.make_request_with_backoff(
                make_request,
                logger=self.logger
            )

            # Check if response is None (all retries failed)
            if response is None:
                self.logger.error(f"Failed to fetch {bill_info} after multiple attempts")
                return None

            # Raise exception for non-200 status codes
            response.raise_for_status()

            data = response.json()

            if summaries := data.get("summaries"):
                if summary_text := summaries[0].get("text", "").strip():
                    return BillSummary(
                        congress=congress,
                        bill_number=f"{chamber}_{number}",
                        summary_text=summary_text,
                        year=self._congress_to_year(int(congress))
                    )

            return None

        except Exception as e:
            self.logger.warning(f"Error fetching {bill_info}: {e}")
            return None

    def _process_public_law(self, congress: int, bill: Dict) -> Optional[BillSummary]:
        """
        Process a single public law bill.

        :param congress: Congress number
        :param bill: Bill dictionary from API response
        :return: BillSummary object or None if processing fails
        """
        try:
            bill_num = bill.get("number", "")
            bill_type = bill.get("type", "").lower()

            api_url = f"{self.API_BASE_URL}/bill/{congress}/{bill_type}/{bill_num}/summaries"

            # Wrap the request in a function for rate limit handler
            def make_request():
                return requests.get(
                    api_url,
                    params={"api_key": self.api_key, "format": "json"}
                )

            # Use rate limit handler to make the request
            summary_response = self.rate_limit_handler.make_request_with_backoff(
                make_request,
                logger=self.logger
            )

            # Check if response is None (all retries failed)
            if summary_response is None:
                self.logger.error(f"Failed to fetch summary for bill {bill_num} after multiple attempts")
                return None

            summary_response.raise_for_status()

            if summaries := summary_response.json().get("summaries"):
                if summary_text := summaries[0].get("text", "").strip():
                    return BillSummary(
                        congress=str(congress),
                        bill_number=f"{bill_type}_{bill_num}",
                        summary_text=summary_text,
                        year=self._congress_to_year(congress)
                    )
            return None

        except Exception as e:
            self.logger.warning(f"Error processing bill {bill_num}: {e}")
            return None

    def _fetch_public_laws(self, congress: int) -> Optional[List[BillSummary]]:
        """
        Fetch public laws for a specific congress.

        :param congress: Congress number
        :return: List of BillSummary objects or None if fetch fails
        """
        summaries = []
        offset = 0
        limit = 250

        while True:
            try:
                api_url = f"{self.API_BASE_URL}/law/{congress}"
                params = {
                    "api_key": self.api_key,
                    "format": "json",
                    "limit": limit,
                    "offset": offset
                }

                # Wrap the request in a function for rate limit handler
                def make_request():
                    return requests.get(api_url, params=params)

                # Use rate limit handler to make the request
                response = self.rate_limit_handler.make_request_with_backoff(
                    make_request,
                    logger=self.logger
                )

                # Check if response is None (all retries failed)
                if response is None:
                    self.logger.error(f"Failed to fetch laws for congress {congress} after multiple attempts")
                    return None  # Signal rate limit or failure

                # Check for rate limit response
                if response.status_code == 429:
                    return None  # Signal rate limit

                response.raise_for_status()
                data = response.json()

                if not (bills := data.get("bills")):
                    break

                for bill in bills:
                    if summary := self._process_public_law(congress, bill):
                        summaries.append(summary)

                offset += limit
                if len(bills) < limit:
                    break

            except Exception as e:
                self.logger.error(f"Error fetching congress {congress}: {e}")
                break

        return summaries

    def analyze_legislation(self, congresses: List[int], selected_bills: List[str]) -> Dict[str, Any]:
        """Analyze legislation with separate caching for analysis results."""
        # First, get the raw data (using shared cache)
        selected_summaries, public_law_summaries = self.fetch_data(congresses, selected_bills)

        # Calculate a cache key based on the selected bills
        selected_bills_key = ','.join(sorted(selected_bills))
        analysis_cache = self._get_analysis_cache_path().with_name(f'analysis_{self.random_seed}_{hash(selected_bills_key)}.json')

        # Check for cached analysis results for this seed and bill selection
        if analysis_cache.exists():
            try:
                with analysis_cache.open() as f:
                    cached_analysis = json.load(f)
                    if self._validate_analysis_cache(cached_analysis):
                        self.logger.info(f"Loading analysis results for seed {self.random_seed}...")
                        # Convert temporal distribution back to DataFrame if needed
                        if isinstance(cached_analysis['temporal_distribution'], dict):
                            cached_analysis['temporal_distribution'] = pd.DataFrame.from_dict(
                                cached_analysis['temporal_distribution'],
                                orient='index'
                            )
                        # Generate visualizations after loading cached results
                        self._create_visualizations(cached_analysis, selected_bills)
                        return cached_analysis
            except Exception as e:
                self.logger.error(f"Analysis cache error: {e}")

        # Perform new analysis
        self.logger.info(f"Performing new analysis with seed {self.random_seed}...")
        analysis_results = self._process_and_analyze(selected_summaries, public_law_summaries, congresses)

        # Convert DataFrame to dict for JSON serialization
        json_results = analysis_results.copy()
        json_results['temporal_distribution'] = {
            str(idx): {
                'reference': int(row['reference']),
                'related': int(row['related']),
                'total': int(row['total']),
                'related_docs': row['related_docs']
            }
            for idx, row in analysis_results['temporal_distribution'].iterrows()
        }

        # Cache analysis results
        with analysis_cache.open('w') as f:
            json.dump(json_results, f, indent=2)

        # Generate visualizations for new analysis
        self._create_visualizations(analysis_results, selected_bills)

        # Log the location of the generated reports
        viz_dir = self.data_dir / 'visualizations'
        self.logger.info(f"\nVisualization outputs saved to:")
        self.logger.info(f"HTML Report: {viz_dir / 'analysis_report.html'}")
        self.logger.info(f"Network Graph: {viz_dir / 'legislation_network.svg'}")

        return analysis_results

    def _process_and_analyze(self,
                       selected_summaries: List[BillSummary],
                       public_law_summaries: List[BillSummary],
                       congresses: List[int] = None) -> Dict[str, Any]:
        """Process and analyze the fetched documents."""
        self.logger.info(f"Processing and analyzing {len(selected_summaries)} selected bills...")

        # Log the bills being analyzed
        for summary in selected_summaries:
            self.logger.info(f"Analyzing bill: {summary.congress}-{summary.bill_number}")

        # Separate reference and comparison documents
        reference_docs = [
            (f"REF_{s.bill_number}", s.year, s.summary_text)
            for s in selected_summaries
        ]
        comparison_docs = [
            (f"PL_{s.bill_number}", s.year, s.summary_text)
            for s in public_law_summaries
        ]

        # Process documents
        ref_processed = self._create_corpus(reference_docs)
        comp_processed = self._create_corpus(comparison_docs)

        # Build dictionary and model
        dictionary = self._build_dictionary(ref_processed + comp_processed)
        ref_processed = self._create_bow_corpus(ref_processed, dictionary)
        comp_processed = self._create_bow_corpus(comp_processed, dictionary)

        # Train LSI model - num_topics will be calculated automatically
        lsi_model = self._train_lsi_model(ref_processed, dictionary)

        # Transform documents
        ref_processed = self._apply_lsi(ref_processed, lsi_model)
        comp_processed = self._apply_lsi(comp_processed, lsi_model)

        # Extract just the bill numbers from selected summaries
        selected_bill_ids = [f"{s.congress}-{s.bill_number}" for s in selected_summaries]

        # Find similar documents
        similar_docs = self._find_similar_documents(
            ref_processed,
            comp_processed,
            threshold=0.75,
            selected_bills=selected_bill_ids
        )

        # Extract topics
        topics = self._extract_topics(lsi_model)

        # Create temporal distribution with congress range
        temporal_dist = self._create_temporal_distribution(ref_processed, similar_docs, congresses)

        return {
            'topics': topics,
            'related_documents': similar_docs,
            'temporal_distribution': temporal_dist
        }

    def _extract_topics(self, lsi_model: LsiModel, num_terms: int = 10) -> List[Dict[str, Any]]:
        """
        Extract readable topics from LSI model.

        :param lsi_model: Trained LSI model
        :param num_terms: Number of terms to extract per topic
        :return: List of topic dictionaries
        """
        topics = []
        for topic_id in range(lsi_model.num_topics):
            terms = lsi_model.show_topic(topic_id, num_terms)
            topics.append({
                'topic_id': topic_id,
                'terms': [(term, float(weight)) for term, weight in terms]
            })
        return topics

    def _create_financial_regulation_embedding(self, tokens: List[str]) -> np.ndarray:
        """
        Create a specialized embedding for financial regulation documents

        Hierarchical Semantic Embedding Strategy:
        1. Core Financial Regulation Domain
        2. Weighted Term Categorization
        3. Contextual Semantic Scoring
        """
        # Hierarchical Financial Regulation Ontology
        domain_hierarchy = {
            'regulatory_core': {
                'terms': {
                    'regulation', 'oversight', 'compliance', 'reform',
                    'governance', 'supervision', 'enforcement',
                    'dodd', 'frank', 'sarbanes', 'oxley', 'basel',
                    'financial', 'securities', 'exchange', 'commission'
                },
                'weight': 3.0
            },
            'financial_instruments': {
                'terms': {
                    'bank', 'securities', 'stock', 'bond', 'derivative',
                    'mortgage', 'credit', 'investment', 'capital', 'market',
                    'loan', 'asset', 'liability', 'equity', 'debt'
                },
                'weight': 2.5
            },
            'risk_management': {
                'terms': {
                    'risk', 'systemic', 'prudential', 'liquidity',
                    'leverage', 'stress', 'capital', 'solvency',
                    'exposure', 'mitigation', 'buffer', 'concentration'
                },
                'weight': 2.0
            },
            'consumer_protection': {
                'terms': {
                    'consumer', 'protection', 'disclosure', 'fair',
                    'transparency', 'rights', 'privacy', 'unfair',
                    'deceptive', 'reporting', 'accountability'
                },
                'weight': 1.5
            },
            'historical_context': {
                'terms': {
                    '1930s', 'depression', 'new deal',
                    '1980s', 'deregulation', 'savings', 'loan',
                    '2000s', 'financial crisis', 'bailout',
                    'fdic', 'firrea', 'volcker', 'gramm', 'leach', 'bliley'
                },
                'weight': 1.0
            }
        }

        # Create semantic vector
        embedding = np.zeros(len(domain_hierarchy))

        for idx, (domain, config) in enumerate(domain_hierarchy.items()):
            # Calculate domain relevance
            domain_tokens = set(config['terms'])
            domain_overlap = len(set(tokens) & domain_tokens)

            # Weighted scoring with exponential bonus
            if domain_overlap > 0:
                embedding[idx] = config['weight'] * (1 + np.log1p(domain_overlap))

        # Normalize embedding
        return embedding / (np.linalg.norm(embedding) + 1e-10)

    def _advanced_document_similarity(self, ref_docs: List[ProcessedDocument], comp_doc: ProcessedDocument) -> float:
        """
        Advanced similarity calculation with multi-stage filtering

        Stages:
        1. Domain-Specific Embedding Similarity
        2. Contextual Semantic Analysis
        3. Temporal Relevance
        4. Domain Filtering
        """
        # Create embeddings for reference and comparison documents
        ref_embeddings = [self._create_financial_regulation_embedding(doc.tokens) for doc in ref_docs]
        comp_embedding = self._create_financial_regulation_embedding(comp_doc.tokens)

        # Calculate mean reference embedding
        mean_ref_embedding = np.mean(ref_embeddings, axis=0)

        # Cosine similarity with embedding
        embedding_sim = np.dot(mean_ref_embedding, comp_embedding) / (
            np.linalg.norm(mean_ref_embedding) * np.linalg.norm(comp_embedding) + 1e-10
        )

        # Domain-Specific Term Overlap
        financial_terms = {
            'core': {
                'terms': {'dodd', 'frank', 'sarbanes', 'oxley', 'basel',
                          'regulation', 'reform', 'financial', 'securities'},
                'weight': 3.0
            },
            'financial': {
                'terms': {'bank', 'securities', 'market', 'investment',
                          'capital', 'loan', 'credit', 'derivative'},
                'weight': 2.0
            },
            'risk': {
                'terms': {'risk', 'systemic', 'liquidity', 'leverage',
                          'exposure', 'prudential', 'solvency'},
                'weight': 1.5
            }
        }

        term_overlaps = {}
        for domain, config in financial_terms.items():
            domain_overlap = len(set(comp_doc.tokens) & config['terms'])
            term_overlaps[domain] = domain_overlap * config['weight']

        # Normalized term similarity
        term_sim = sum(term_overlaps.values()) / (len(comp_doc.tokens) + 1e-10)

        # Temporal Relevance (Gentle Decay)
        ref_years = [doc.year for doc in ref_docs]
        avg_ref_year = np.mean(ref_years)
        time_decay = np.exp(-0.023 * abs(comp_doc.year - avg_ref_year) / 30)

        # Combined Similarity with Domain-Specific Weighting
        similarity = (
            0.5 * embedding_sim +  # Embedding Similarity
            0.3 * term_sim +        # Term Overlap
            0.2 * time_decay        # Temporal Relevance
        )

        # Strict Domain Filtering
        financial_threshold = 0.5
        if similarity < financial_threshold:
            return 0.0

        return similarity

    def _classify_document_domain(self, tokens: List[str]) -> Dict[str, float]:
        """
        Classify document domain with probabilistic approach

        :param tokens: Preprocessed document tokens
        :return: Dictionary of domain probabilities
        """
        # Comprehensive domain definitions with hierarchical weighting
        domain_definitions = {
            'financial_regulation': {
                'primary_terms': {
                    'regulation', 'oversight', 'compliance', 'reform',
                    'dodd', 'frank', 'sarbanes', 'oxley', 'basel',
                    'securities', 'exchange', 'financial', 'investment'
                },
                'secondary_terms': {
                    'bank', 'credit', 'capital', 'market', 'risk',
                    'systemic', 'derivative', 'loan', 'mortgage'
                },
                'weights': {
                    'primary': 3.0,
                    'secondary': 1.5
                }
            },
            'appropriations': {
                'primary_terms': {
                    'appropriation', 'budget', 'funding', 'fiscal',
                    'expenditure', 'allocation', 'spending'
                },
                'secondary_terms': {
                    'department', 'agency', 'government', 'finance'
                },
                'weights': {
                    'primary': 2.0,
                    'secondary': 1.0
                }
            },
            'general_legislation': {
                'primary_terms': {
                    'act', 'bill', 'law', 'resolution', 'amendment'
                },
                'secondary_terms': {
                    'congress', 'senate', 'house', 'committee'
                },
                'weights': {
                    'primary': 0.5,
                    'secondary': 0.25
                }
            }
        }

        # Compute domain probabilities
        domain_scores = {}
        token_set = set(tokens)

        for domain, config in domain_definitions.items():
            primary_overlap = len(token_set.intersection(config['primary_terms']))
            secondary_overlap = len(token_set.intersection(config.get('secondary_terms', set())))

            # Probabilistic scoring with exponential decay
            primary_score = primary_overlap * config['weights']['primary']
            secondary_score = secondary_overlap * config['weights']['secondary']

            # Combine scores with diminishing returns
            domain_scores[domain] = np.log1p(primary_score + secondary_score)

        # Normalize probabilities
        total_score = sum(domain_scores.values())
        domain_probabilities = {
            domain: score / (total_score + 1e-10)
            for domain, score in domain_scores.items()
        }

        return domain_probabilities

    def _advanced_semantic_similarity(self,
                                      ref_tokens: List[str],
                                      comp_tokens: List[str]) -> float:
        """
        Advanced semantic similarity with domain-aware filtering

        :param ref_tokens: Tokens from reference documents
        :param comp_tokens: Tokens from comparison document
        :return: Semantic similarity score
        """
        # Domain classification for both reference and comparison
        ref_domain = self._classify_document_domain(ref_tokens)
        comp_domain = self._classify_document_domain(comp_tokens)

        # Strict financial regulation domain filtering
        if ref_domain['financial_regulation'] < 0.5 or comp_domain['financial_regulation'] < 0.5:
            return 0.0

        # Enhanced term overlap with domain-specific weighting
        financial_terms = {
            'critical': {
                'terms': {'dodd', 'frank', 'sarbanes', 'oxley', 'basel',
                          'regulation', 'reform', 'financial', 'securities'},
                'weight': 3.0
            },
            'important': {
                'terms': {'bank', 'investment', 'capital', 'market',
                          'risk', 'systemic', 'derivative', 'loan'},
                'weight': 2.0
            }
        }

        # Compute weighted term overlap
        weighted_overlap = 0.0
        total_weight = 0.0

        for category, config in financial_terms.items():
            overlap = len(set(ref_tokens).intersection(config['terms']))
            weighted_overlap += overlap * config['weight']
            total_weight += config['weight']

        # Normalize and scale overlap
        normalized_overlap = weighted_overlap / (total_weight + 1e-10)

        # Combine domain probability with term overlap
        similarity = (
            0.6 * ref_domain['financial_regulation'] +
            0.4 * normalized_overlap
        )

        return max(0.0, min(1.0, similarity))

    def _find_similar_documents(self,
                          ref_docs: List[ProcessedDocument],
                          comp_docs: List[ProcessedDocument],
                          threshold: float = 0.7,
                          selected_bills: Optional[List[str]] = None) -> List[Dict[str, Any]]:
        """
        Enhanced document similarity finder with strict domain filtering

        :param ref_docs: Reference documents
        :param comp_docs: Comparison documents
        :param threshold: Similarity threshold
        :param selected_bills: Bills to exclude
        :return: List of similar documents
        """
        # Prepare excluded bills set
        excluded_bills = set(selected_bills or [])

        # Collect reference tokens
        ref_tokens = []
        for doc in ref_docs:
            ref_tokens.extend(doc.tokens)

        # Compute domain characteristics of reference documents
        ref_domain_probs = self._classify_document_domain(ref_tokens)

        # Verify strong financial regulation focus
        if ref_domain_probs['financial_regulation'] < 0.5:
            self.logger.warning("Reference documents lack financial regulation focus")
            return []

        # Compute similarities
        similarities = []
        for doc in comp_docs:
            # Skip excluded bills
            if doc.original_id in excluded_bills:
                continue

            # Compute advanced semantic similarity
            similarity = self._advanced_semantic_similarity(ref_tokens, doc.tokens)

            # Apply threshold and collect results
            if similarity > threshold:
                similarities.append({
                    'doc_id': doc.original_id,
                    'year': doc.year,
                    'similarity': similarity,
                    'summary_text': getattr(doc, 'summary_text', ''),
                    'domain_probs': self._classify_document_domain(doc.tokens)
                })

        # Sort and return top results
        return sorted(
            similarities,
            key=lambda x: x['similarity'],
            reverse=True
        )[:15]

    def _calculate_topic_similarity(self, ref_docs: List[ProcessedDocument],
                                comp_doc: ProcessedDocument) -> float:
        """
        Calculate topic similarity using advanced distribution comparison.

        :param ref_docs: Reference documents
        :param comp_doc: Comparison document
        :return: Topic similarity score
        """
        # Aggregate reference topic distributions
        ref_topic_dists = [doc.topic_dist for doc in ref_docs if doc.topic_dist is not None]

        if not ref_topic_dists:
            return 0.0

        # Calculate mean reference distribution
        mean_ref_dist = np.mean(ref_topic_dists, axis=0)

        # Compute similarity using Jensen-Shannon divergence
        try:
            # Safely handle comparison distribution
            if comp_doc.topic_dist is None or np.all(comp_doc.topic_dist == 0):
                return 0.0

            # Ensure non-negative and properly normalized distributions
            mean_ref_dist = np.abs(mean_ref_dist)
            mean_ref_dist = mean_ref_dist / (mean_ref_dist.sum() + 1e-10)

            comp_dist = np.abs(comp_doc.topic_dist)
            comp_dist = comp_dist / (comp_dist.sum() + 1e-10)

            # Lower Jensen-Shannon divergence means higher similarity
            js_divergence = jensenshannon(mean_ref_dist, comp_dist)

            # Convert divergence to similarity (1 - divergence)
            return 1 - js_divergence

        except Exception as e:
            self.logger.warning(f"Topic similarity calculation error: {e}")
            import traceback
            traceback.print_exc()
            return 0.0

            # Calculate mean reference distribution
            mean_ref_dist = np.mean(ref_topic_dists, axis=0)

            # Compute similarity using Jensen-Shannon divergence
            try:
                # Ensure non-negative and properly normalized distributions
                mean_ref_dist = np.abs(mean_ref_dist)
                mean_ref_dist /= mean_ref_dist.sum()

                comp_dist = np.abs(comp_doc.topic_dist)
                comp_dist /= comp_dist.sum()

                # Lower Jensen-Shannon divergence means higher similarity
                js_divergence = jensenshannon(mean_ref_dist, comp_dist)

                # Convert divergence to similarity (1 - divergence)
                return 1 - js_divergence

            except Exception as e:
                self.logger.warning(f"Topic similarity calculation error: {e}")
                return 0.0

    def _create_advanced_financial_ontology(self) -> Dict[str, Dict]:
        """
        Create a sophisticated, hierarchical financial regulation ontology.

        Structured as a nested dictionary with:
        - Primary domains
        - Secondary concepts
        - Weighted importance
        - Historical context
        """
        return {
            'regulatory_framework': {
                'weight': 3.0,
                'primary_terms': {
                    'regulation', 'oversight', 'compliance', 'reform',
                    'governance', 'supervision', 'enforcement'
                },
                'secondary_terms': {
                    'transparency', 'accountability', 'disclosure',
                    'reporting', 'audit', 'examination'
                },
                'historical_variants': {
                    '1930s': {'terms': {'depression', 'new deal', 'securities'}},
                    '1980s': {'terms': {'deregulation', 'savings', 'loan'}},
                    '2000s': {'terms': {'dodd-frank', 'systemic', 'bailout'}}
                }
            },
            'financial_institutions': {
                'weight': 2.5,
                'primary_terms': {
                    'bank', 'credit', 'investment', 'securities',
                    'exchange', 'broker', 'dealer', 'lender'
                },
                'secondary_terms': {
                    'corporation', 'firm', 'institution', 'company',
                    'financial', 'capital', 'market'
                }
            },
            'financial_instruments': {
                'weight': 2.0,
                'primary_terms': {
                    'loan', 'mortgage', 'derivative', 'security',
                    'stock', 'bond', 'equity', 'debt', 'credit'
                },
                'secondary_terms': {
                    'investment', 'trade', 'transaction', 'asset',
                    'liability', 'portfolio'
                }
            },
            'risk_management': {
                'weight': 1.5,
                'primary_terms': {
                    'risk', 'systemic', 'prudential', 'stress',
                    'liquidity', 'capital', 'solvency'
                },
                'secondary_terms': {
                    'exposure', 'mitigation', 'threshold', 'buffer',
                    'leverage', 'concentration'
                }
            },
            'consumer_protection': {
                'weight': 1.0,
                'primary_terms': {
                    'consumer', 'protection', 'fair', 'disclosure',
                    'transparency', 'rights', 'privacy'
                }
            }
        }

    def _advanced_semantic_similarity(self,
                                      ref_tokens: List[str],
                                      comp_tokens: List[str]) -> float:
        """
        Advanced semantic similarity calculation using ontology-based weighting.

        :param ref_tokens: Tokens from reference documents
        :param comp_tokens: Tokens from comparison document
        :return: Semantic similarity score
        """
        ontology = self._create_advanced_financial_ontology()

        # Initialize similarity components
        domain_similarities = {}

        for domain, config in ontology.items():
            # Calculate term overlap with weighted importance
            primary_overlap = len(
                set(ref_tokens).intersection(config['primary_terms'])
            ) * config['weight']

            secondary_overlap = 0.5 * len(
                set(ref_tokens).intersection(config.get('secondary_terms', set()))
            )

            # Check historical variants
            historical_overlap = 0
            if 'historical_variants' in config:
                for era, era_config in config['historical_variants'].items():
                    historical_overlap += 0.3 * len(
                        set(ref_tokens).intersection(era_config.get('terms', set()))
                    )

            # Combine overlaps
            domain_similarities[domain] = (
                primary_overlap +
                secondary_overlap +
                historical_overlap
            )

        # Normalize and weight similarities
        total_weight = sum(ontology[domain]['weight'] for domain in domain_similarities)
        weighted_similarity = sum(
            sim * ontology[domain]['weight']
            for domain, sim in domain_similarities.items()
        ) / total_weight

        return weighted_similarity

    def _calculate_term_similarity(self, ref_docs: List[ProcessedDocument],
                                comp_doc: ProcessedDocument) -> float:
        """
        Enhanced term similarity calculation with domain-specific weighting.

        :param ref_docs: Reference documents
        :param comp_doc: Comparison document
        :return: Weighted term similarity score
        """
        # Comprehensive financial regulation term weights
        financial_domain_terms = {
            # High-weight critical terms
            'critical': {
                'terms': {'dodd', 'frank', 'gramm', 'leach', 'bliley', 'sarbanes',
                          'oxley', 'fdic', 'firrea', 'basel', 'volcker', 'rule',
                          'regulation', 'reform', 'oversight', 'compliance'},
                'weight': 3.0
            },
            # Medium-weight important terms
            'important': {
                'terms': {'bank', 'banking', 'financial', 'securities', 'exchange',
                          'commission', 'investment', 'capital', 'credit', 'loan',
                          'mortgage', 'derivative', 'risk', 'systemic', 'market'},
                'weight': 2.0
            },
            # Low-weight supporting terms
            'supporting': {
                'terms': {'institution', 'corporation', 'transaction', 'disclosure',
                          'reporting', 'audit', 'enforcement', 'supervision',
                          'transparency', 'accountability'},
                'weight': 1.0
            }
        }

        # Get unique terms from reference docs
        ref_terms = set()
        for doc in ref_docs:
            ref_terms.update(doc.tokens)

        # Get comparison doc terms
        comp_terms = set(comp_doc.tokens)

        # Calculate weighted intersection
        total_weight = 0.0
        weighted_intersection = 0.0

        for category, config in financial_domain_terms.items():
            category_terms = config['terms']
            category_weight = config['weight']

            # Calculate intersection and weight
            category_intersection = len(ref_terms.intersection(category_terms).intersection(comp_terms))
            total_weight += category_weight
            weighted_intersection += category_intersection * category_weight

        # Calculate Jaccard-like similarity with term weighting
        ref_union = ref_terms.union(comp_terms)

        # Prevent division by zero
        epsilon = 1e-10

        # Combine weighted intersection with traditional Jaccard
        weighted_jaccard = (
            0.7 * (weighted_intersection / (total_weight + epsilon)) +
            0.3 * (len(ref_terms.intersection(comp_terms)) / (len(ref_union) + epsilon))
        )

        return weighted_jaccard

    def _calculate_temporal_weight(self, ref_docs: List[ProcessedDocument],
                                comp_doc: ProcessedDocument) -> float:
        """
        Temporal weight calculation optimized for broad historical comparisons.

        :param ref_docs: Reference documents
        :param comp_doc: Comparison document
        :return: Temporal relevance weight
        """
        # Get years of reference documents
        ref_years = [doc.year for doc in ref_docs]
        avg_ref_year = sum(ref_years) / len(ref_years)

        # Calculate time difference
        time_diff = abs(comp_doc.year - avg_ref_year)

        # Very gentle temporal decay
        # Much longer half-life (30 years instead of 5)
        temporal_weight = np.exp(-0.023263 * (time_diff / 30.0))  # ln(2)/30  0.023263

        # Instead of a hard cutoff, use a gradual reduction
        # Documents within 30 years get minimal penalty
        # Beyond that, gradual reduction
        if time_diff > 30:
            temporal_weight *= 0.7  # Moderate reduction for very distant documents

        # Ensure a minimum weight to allow cross-era comparisons
        return max(temporal_weight, 0.2)

    def _extract_congress_from_summary(self, summary: BillSummary) -> int:
        """
        Extract congress number from BillSummary object.

        :param summary: BillSummary object
        :return: Congress number as integer
        """
        return int(summary.congress)

    def _create_temporal_distribution(self,
                                ref_docs: List[ProcessedDocument],
                                related_docs: List[Dict[str, Any]],
                                congresses: List[int] = None) -> pd.DataFrame:
        """
        Create temporal distribution DataFrame with complete year range and related legislation.

        :param ref_docs: Reference documents
        :param related_docs: Related documents
        :param congresses: List of congress numbers to ensure full range coverage
        :return: DataFrame with temporal distribution and related documents
        """
        # Calculate year range based on congresses if provided
        if congresses:
            min_year = self._congress_to_year(min(congresses))
            max_year = self._congress_to_year(max(congresses)) + 1  # Add 1 to include the second year
        else:
            # Fallback to document years if no congresses provided
            ref_years = [doc.year for doc in ref_docs if hasattr(doc, 'year')]
            related_years = [doc['year'] for doc in related_docs if 'year' in doc]
            all_years = ref_years + related_years
            if not all_years:
                min_year = 1900
                max_year = 2025
            else:
                min_year = min(all_years)
                max_year = max(all_years)

        # Create DataFrame with complete year range
        all_years_df = pd.DataFrame(
            index=pd.Index(range(min_year, max_year + 1), name='year')
        )

        # Initialize all columns with zeros
        all_years_df['reference'] = 0
        all_years_df['related'] = 0
        all_years_df['total'] = 0
        all_years_df['related_docs'] = [[] for _ in range(len(all_years_df))]

        # Count reference documents
        for doc in ref_docs:
            if hasattr(doc, 'year'):
                all_years_df.at[doc.year, 'reference'] = 1

        # Count and collect related documents
        related_counts = defaultdict(int)
        related_docs_by_year = defaultdict(list)

        for doc in related_docs:
            if 'year' in doc:
                year = doc['year']
                related_counts[year] += 1
                related_docs_by_year[year].append(doc['doc_id'])

        # Update related documents counts and lists
        for year in related_counts:
            if year in all_years_df.index:
                all_years_df.at[year, 'related'] = related_counts[year]
                all_years_df.at[year, 'related_docs'] = related_docs_by_year[year]

        # Calculate totals
        all_years_df['total'] = all_years_df['reference'] + all_years_df['related']

        return all_years_df

    def _create_network_graph(self,
                        analysis_results: Dict[str, Any],
                        selected_bills: List[str],
                        output_path: str):
        """
        Create network visualization as SVG instead of HTML.

        :param analysis_results: Dictionary of analysis results
        :param selected_bills: List of selected bills
        :param output_path: Path to save network graph
        """
        try:
            # Create a NetworkX graph
            G = nx.Graph()

            # Add topic nodes
            for topic in analysis_results.get('topics', []):
                topic_id = f"Topic_{topic['topic_id']}"
                top_terms = ", ".join([term for term, _ in topic['terms'][:5]])
                G.add_node(topic_id,
                          node_type='topic',
                          label=f"Topic {topic['topic_id']}\n{top_terms}")

            # Add reference document nodes
            for bill in selected_bills:
                G.add_node(f"REF_{bill}",
                          node_type='reference',
                          label=bill)

            # Add related document nodes and edges
            for doc in analysis_results.get('related_documents', []):
                if doc.get('similarity', 0) >= 0.65:
                    doc_id = doc['doc_id']
                    G.add_node(doc_id,
                              node_type='related',
                              label=f"{doc_id}\n({doc['year']})")

                    # Add edges to topics
                    for topic in analysis_results.get('topics', []):
                        G.add_edge(doc_id, f"Topic_{topic['topic_id']}")

            # Calculate layout
            pos = nx.spring_layout(G, k=1, iterations=50)

            # Create SVG
            plt.figure(figsize=(20, 20))

            # Draw nodes by type with different colors and sizes
            topic_nodes = [n for n, d in G.nodes(data=True) if d['node_type'] == 'topic']
            ref_nodes = [n for n, d in G.nodes(data=True) if d['node_type'] == 'reference']
            related_nodes = [n for n, d in G.nodes(data=True) if d['node_type'] == 'related']

            nx.draw_networkx_nodes(G, pos, nodelist=topic_nodes, node_color='#44ff44',
                                node_size=3000, alpha=0.8)
            nx.draw_networkx_nodes(G, pos, nodelist=ref_nodes, node_color='#ff4444',
                                node_size=2500, alpha=0.8, node_shape='s')
            nx.draw_networkx_nodes(G, pos, nodelist=related_nodes, node_color='#4444ff',
                                node_size=2000, alpha=0.8)

            # Draw edges
            nx.draw_networkx_edges(G, pos, alpha=0.3)

            # Add labels
            labels = nx.get_node_attributes(G, 'label')
            nx.draw_networkx_labels(G, pos, labels, font_size=8)

            # Remove axes
            plt.axis('off')

            # Save as SVG
            output_path = output_path.replace('.html', '.svg')
            plt.savefig(output_path, format='svg', bbox_inches='tight')
            plt.close()

            self.logger.info(f"Network graph saved to {output_path}")

        except Exception as e:
            self.logger.error(f"Error creating network graph: {e}")
            # Create a simple error SVG
            error_svg = f'''<?xml version="1.0" encoding="UTF-8"?>
            <svg xmlns="http://www.w3.org/2000/svg" width="400" height="100">
                <text x="10" y="50" font-family="Arial" font-size="14">
                    Error creating network visualization: {str(e)}
                </text>
            </svg>
            '''
            with open(output_path.replace('.html', '.svg'), 'w') as f:
                f.write(error_svg)

    def _create_visualizations(self,
                            analysis_results: Dict[str, Any],
                            selected_bills: List[str]):
        """
        Create visualizations based on analysis results.

        :param analysis_results: Dictionary of analysis results
        :param selected_bills: List of selected bills
        """
        # Create output directory
        viz_dir = self.data_dir / 'visualizations'
        viz_dir.mkdir(parents=True, exist_ok=True)

        # Generate network graph
        network_path = viz_dir / 'legislation_network.svg'  # Changed to .svg
        self._create_network_graph(analysis_results, selected_bills, str(network_path))

        # Generate HTML report
        report_path = viz_dir / 'analysis_report.html'
        self._generate_report(analysis_results, str(report_path))

        self.logger.info(f"Visualizations saved in {viz_dir}")

    def clean_caches(self, keep_latest_analyses: int = 5) -> None:
        """Clean up analysis caches while preserving raw data cache."""
        try:
            analysis_files = list(self.analysis_cache_dir.glob('analysis_*.json'))
            analysis_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)

            for cache_file in analysis_files[keep_latest_analyses:]:
                try:
                    cache_file.unlink()
                    self.logger.info(f"Removed old analysis cache: {cache_file}")
                except Exception as e:
                    self.logger.warning(f"Failed to remove analysis cache {cache_file}: {e}")

        except Exception as e:
            self.logger.error(f"Error cleaning cache files: {e}")

    def get_cache_info(self) -> Dict[str, Any]:
        """Get information about both raw and analysis caches."""
        info = {
            'raw_data': None,
            'analysis_results': []
        }

        if self.raw_cache_path.exists():
            stats = self.raw_cache_path.stat()
            with self.raw_cache_path.open() as f:
                data = json.load(f)
                info['raw_data'] = {
                    'size': stats.st_size,
                    'modified': datetime.fromtimestamp(stats.st_mtime),
                    'num_selected': len(data.get('selected', [])),
                    'num_public_laws': len(data.get('public_laws', [])),
                    'cache_date': data.get('cache_date')
                }

        for cache_file in self.analysis_cache_dir.glob('analysis_*.json'):
            stats = cache_file.stat()
            seed = int(cache_file.stem.split('_')[1])
            info['analysis_results'].append({
                'seed': seed,
                'size': stats.st_size,
                'modified': datetime.fromtimestamp(stats.st_mtime)
            })

        return info

    def _generate_report(self, analysis_results: Dict[str, Any], output_path: str):
        """
        Generate HTML analysis report with bill summaries.

        :param analysis_results: Dictionary of analysis results
        :param output_path: Path to save HTML report
        """
        # Filter out topics with no significant terms
        significant_topics = [
            topic for topic in analysis_results['topics']
            if any(weight > 0.01 for _, weight in topic['terms'])
        ]

        report = f"""
            <html>
            <head>
                <title>Legislative Analysis Report</title>
                <style>
                    body {{ font-family: Arial, sans-serif; margin: 40px; }}
                    .topic {{ margin-bottom: 20px; }}
                    .related-doc {{ margin-bottom: 20px; padding: 10px; background-color: #f8f9fa; border-radius: 4px; }}
                    table {{ border-collapse: collapse; width: 100%; }}
                    th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; vertical-align: top; }}
                    th {{ background-color: #f2f2f2; }}
                    .related-list {{ font-size: 0.9em; color: #666; margin-top: 5px; }}
                    .year-row {{ background-color: #fff; }}
                    .year-row:nth-child(even) {{ background-color: #f9f9f9; }}
                    .empty-year {{ color: #999; }}
                    .congress-span {{ font-size: 0.9em; color: #666; }}
                    .bill-count {{ font-weight: bold; }}
                    .bill-summary {{ margin-top: 5px; font-size: 0.9em; color: #666; }}
                    .congress-group {{ margin: 5px 0; padding: 5px; background-color: #f5f5f5; border-radius: 4px; }}
                    .congress-header {{ font-weight: bold; color: #444; margin-bottom: 3px; }}
                </style>
            </head>
            <body>
                <h1>Legislative Analysis Report</h1>

                <h2>Key Topics Identified ({len(significant_topics)} topics)</h2>
                {''.join(f'''
                <div class="topic">
                    <h3>Topic {i+1}</h3>
                    <p>Key terms: {', '.join(f"{term} ({weight:.3f})" for term, weight in topic['terms'] if weight > 0.01)}</p>
                </div>
                ''' for i, topic in enumerate(significant_topics))}

                <h2>Most Related Documents</h2>
                {''.join(f'''
                <div class="related-doc">
                    <p><strong>{doc['doc_id']}</strong> (Year: {doc['year']})
                    <br>Similarity: {doc['similarity']:.3f}</p>
                    <div class="bill-summary">{self._truncate_summary(doc.get('summary_text', 'No summary available.'))}</div>
                </div>
                ''' for doc in analysis_results['related_documents'][:10])}

                <h2>Temporal Distribution</h2>
                <table>
                    <tr>
                        <th>Year</th>
                        <th>Congress</th>
                        <th>Reference Bills</th>
                        <th>Related Bills</th>
                        <th>Total</th>
                        <th>Related Legislation</th>
                    </tr>
                    {''.join(f'''
                    <tr class="year-row {'' if row['total'] > 0 else 'empty-year'}">
                        <td>{year}</td>
                        <td>
                            <span class="congress-span">{((year - 1789) // 2) + 1}th Congress
                            {" (First Year)" if year % 2 == 1 else " (Second Year)"}</span>
                        </td>
                        <td>{int(row['reference'])}</td>
                        <td>{int(row['related'])}</td>
                        <td>{int(row['total'])}</td>
                        <td>
                            {self._format_related_legislation(row['related_docs'], year)}
                        </td>
                    </tr>
                    ''' for year, row in analysis_results['temporal_distribution'].iterrows())}
                </table>
            </body>
            </html>
            """

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report)

    def _format_related_legislation(self, docs: List[str], year: int) -> str:
        """
        Format related legislation list for a specific year.

        :param docs: List of related document IDs
        :param year: Year for the row
        :return: Formatted HTML string
        """
        if not docs:
            return '<div class="related-list">None</div>'

        # Calculate congress number for this year
        congress_number = str((year - 1789) // 2 + 1)

        # Filter bills for this year's congress
        year_bills = []
        for doc in docs:
            try:
                bill_parts = doc.split('_')
                if bill_parts[0] == 'PL':
                    # For PL documents, add them directly
                    year_bills.append(doc)
                else:
                    # For other documents, check congress number
                    congress = ''.join(filter(str.isdigit, bill_parts[0]))
                    if congress == congress_number:
                        year_bills.append(doc)
            except Exception:
                continue

        if not year_bills:
            return '<div class="related-list">None</div>'

        return f'''
        <div class="related-list">
            {", ".join(year_bills)}
        </div>
        '''

    def _truncate_summary(self, text: str, max_length: int = 600) -> str:
        """
        Truncate summary text at a sentence boundary, not exceeding max_length.
        Preserves full text if under max_length.

        :param text: Text to truncate
        :param max_length: Maximum length of returned string
        :return: Original text if under max_length, otherwise truncated text with ellipsis
        """
        if not text:
            return "No summary available."

        # If text is already short enough, return it as is
        if len(text) <= max_length:
            return text

        # Find the last sentence boundary before max_length
        shortened = text[:max_length]
        sentences = []
        for delimiter in ['. ', '! ', '? ']:
            last_point = shortened.rfind(delimiter)
            if last_point != -1:
                sentences.append(last_point + len(delimiter) - 1)

        # If we found any sentence boundaries, use the latest one
        if sentences:
            end_point = max(sentences)
            return text[:end_point] + '...'

        # If no sentence boundary found, cut at max_length and ensure we don't cut mid-word
        return shortened.rsplit(' ', 1)[0] + '...'


# Main Call
if __name__ == "__main__":
    # Prompt for API key securely
    api_key = getpass.getpass(prompt="Enter your Congress.gov API key: ")

    # Initialize the analyzer
    analyzer = LegislativeAnalyzer(api_key)

    import shutil
    if analyzer.analysis_cache_dir.exists():
        shutil.rmtree(analyzer.analysis_cache_dir)
        print("Cleared analysis cache directory")
    analyzer.analysis_cache_dir.mkdir(parents=True, exist_ok=True)
    print("Created fresh analysis cache directory")

    # Define congresses and bills to analyze
    congresses = list(range(93,118))
    selected_bills = [
    # Modern Financial Regulation
    '111-hr_4173',  # Dodd-Frank Wall Street Reform
    '106-s_900',    # Gramm-Leach-Bliley Act
    '101-hr_1278',  # FIRREA
    '97-hr_6267',   # Garn-St. Germain Depository Institutions Act
    '96-hr_4986',   # Depository Institutions Deregulation and Monetary Control Act

    # Banking Safety & Soundness
    '102-s_543',    # FDICIA

    # Consumer Protection
    '106-hr_4585',  # E-Sign Act

    # Securities Regulation
    '107-s_2673',   # Sarbanes-Oxley Act

    ]
    try:
        # Run the analysis
        results = analyzer.analyze_legislation(congresses, selected_bills)

        print("\nAnalysis complete! Check the 'cache_data/visualizations' directory for outputs.")

        # Print some basic analysis results
        print("\n--- Analysis Summary ---")

        # Print Topics
        print("\nIdentified Topics:")
        for topic in results['topics']:
            print(f"Topic {topic['topic_id']}:")
            for term, weight in topic['terms'][:5]:
                print(f"  - {term} (weight: {weight:.3f})")

        # Print Related Documents
        print("\nMost Related Documents:")
        for doc in results['related_documents'][:5]:
            print(f"  - {doc['doc_id']} (Year: {doc['year']}, Similarity: {doc['similarity']:.3f})")

        # Print Temporal Distribution
        print("\nTemporal Distribution:")
        temporal_dist = results['temporal_distribution']
        print(temporal_dist)

    except Exception as e:
        print(f"An error occurred during analysis: {e}")
        import traceback
        traceback.print_exc()